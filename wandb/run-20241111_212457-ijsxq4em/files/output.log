100%|██████████████████████████████████████████████████| 10/10 [00:00<00:00, 40.68it/s, Warming up...]                                                                                 
  0%|                                                  | 0/2000000 [00:00<?, ?it/s]                                                                                                    /home/skanda/InterIIT/MADDPG/maddpg/main.py:44: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  nobs, rwds, dones, _ = env.step(np.array(acts))
/home/skanda/InterIIT/MADDPG/maddpg/main.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  nobs, rwds, dones, _ = env.step(np.array(acts))
  0%|                                                  | 0/2000000 [00:41<?, ?it/s]
Traceback (most recent call last):
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 412, in <module>
    _ = train(config, wandb_run)
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 246, in train
    tr, _, col = play_episode(
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 43, in play_episode
    acts = action_fn([obs, env.goals, env.currentPositions])
  File "/home/skanda/InterIIT/MADDPG/maddpg/maddpg.py", line 58, in acts
    tempObs = torch.cat((torch.Tensor(tempObs), torch.Tensor(goals[ii]).unsqueeze(0), torch.Tensor(states[ii]).unsqueeze(0)), dim=1)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 412, in <module>
    _ = train(config, wandb_run)
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 246, in train
    tr, _, col = play_episode(
  File "/home/skanda/InterIIT/MADDPG/maddpg/main.py", line 43, in play_episode
    acts = action_fn([obs, env.goals, env.currentPositions])
  File "/home/skanda/InterIIT/MADDPG/maddpg/maddpg.py", line 58, in acts
    tempObs = torch.cat((torch.Tensor(tempObs), torch.Tensor(goals[ii]).unsqueeze(0), torch.Tensor(states[ii]).unsqueeze(0)), dim=1)
KeyboardInterrupt
